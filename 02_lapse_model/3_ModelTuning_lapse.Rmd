---
title: '**Lapse: Model tuning and selection**'
author: '*Roberto Jes√∫s Alcaraz Molina*'
date: "25/05/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    df_print: paged
header-includes: 
- \usepackage{float}
- \usepackage{amsbsy}
- \usepackage{amsmath}
- \usepackage{graphicx}
- \usepackage{subfig}
- \usepackage{booktabs}
---

```{r include=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = T, 
                      message = FALSE,
                      fig.pos="H", 
                      fig.align="center",
                      fig.width=15,
                      cache=FALSE, error = TRUE)
```

```{r, echo = F}
# install.packages("pacman")
# devtools::install_github("stevenpawley/recipeselectors")
pacman::p_load(tidyverse, tidymodels, workflowsets, tune, patchwork, dotwhisker, doParallel, mgcv, performance, recipeselectors, vip)
theme_set(theme_bw())

# Models packages
pacman::p_load(ranger)
```

```{r}
lapse_data <- readRDS("../00_data/insurance_lapse.RDS")
lapse_data$lapse <- as.factor(lapse_data$lapse)

lapse_data <- lapse_data %>%
  filter(actuarial_age >= 21, actuarial_age <= 61)

set.seed(123)

lapse_split <- initial_split(lapse_data, prop = 0.7, strata = lapse)
lapse_train <- training(lapse_split)
lapse_test  <- testing(lapse_split)

set.seed(23)
lapse_folds <- vfold_cv(lapse_train, v = 5,
                            strata = lapse)

basic_rec <- 
  recipe(lapse ~ sex + duration + actuarial_age + IMC + capital, 
         data = lapse_train) %>%
  step_log(capital, IMC, base = 10) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ actuarial_age:sex_woman + duration:sex_woman + IMC:sex_woman + capital:sex_woman)

basic_rec %>% prep() %>% juice
```

# Metrics
```{r}
grid_ctrl <- control_grid(
  save_pred = TRUE, 
  save_workflow = TRUE, 
  event_level = "second"
  )

f_meas_2 <- function(data, truth, estimate, na_rm = TRUE, ...){
  f_meas(
    data = data,
    truth = !! rlang::enquo(truth),
    estimate = !! rlang::enquo(estimate),
    # set beta = 2
    beta = 2,
    na_rm = TRUE,
    ...
  )
}

f_meas_2 <- new_class_metric(f_meas_2, direction = "maximize")

# It works
# prob %>%
#     mutate(pred_class = as.factor(ifelse(value > t[i], "yes", "no"))) %>%
#     f_meas(lapse, pred_class, beta = 2, event_level = "second")
# prob %>%
#     mutate(pred_class = as.factor(ifelse(value > t[i], "yes", "no"))) %>%
#     f_meas_2(lapse, pred_class)

my_metrics = metric_set(f_meas_2, recall, kap)
```


# Models

```{r}
xgboost_rec <- 
  recipe(lapse ~ sex + duration + actuarial_age + IMC + capital, 
         data = lapse_train) %>%
  step_log(capital, IMC, base = 10) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_interact(terms = ~ actuarial_age:sex_woman + duration:sex_woman + IMC:sex_woman + capital:sex_woman + actuarial_age:sex_man + duration:sex_man + IMC:sex_man + capital:sex_man) %>% 
  step_zv(all_predictors())

xgboost_model <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 

xgboost_wf <- 
  workflow() %>% 
  add_recipe(xgboost_rec) %>% 
  add_model(xgboost_model) 
```

```{r}
# Random Forest
rf_rec <- 
  recipe(lapse ~ sex + duration + actuarial_age + IMC + capital, 
         data = lapse_train) %>%
  step_log(capital, IMC, base = 10) %>%
  step_interact(terms = ~ actuarial_age:sex + duration:sex + IMC:sex + capital:sex)

rf_model <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

rf_wf <- 
  workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_model)
```


```{r}
mlp_rec <- 
  recipe(lapse ~ sex + duration + actuarial_age + IMC + capital, 
         data = lapse_train) %>%
  step_log(capital, IMC, base = 10) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ actuarial_age:sex_woman + duration:sex_woman + IMC:sex_woman + capital:sex_woman) %>% 
  step_zv(all_predictors()) %>%
  step_corr(all_predictors())

mlp_model <-
  mlp(hidden_units = tune(), epochs = tune(), penalty = tune()) %>%
  set_engine('nnet') %>%
  set_mode('classification')

mlp_wf <- 
  workflow() %>%
  add_recipe(mlp_rec) %>%
  add_model(mlp_model)
```


# Parameter tuning

```{r}
# GAM
n <- nrow(lapse_folds)

f2 <- seq(1, n)
kappa <- seq(1, n)
rec <- seq(1, n)
t <- Sys.time()

for (i in 1:n){
  
  analysis_data <- analysis(lapse_folds$splits[[i]])
  analysis_data <- 
    recipe(lapse ~ sex + actuarial_age + duration + capital + IMC,
           data = analysis_data) %>%
    step_log(capital, IMC, base = 10) %>% 
    prep() %>% juice()
  
  validation_data <- assessment(lapse_folds$splits[[i]])
  
  gam_model_lapse <- gam(lapse ~ sex +
                         s(actuarial_age, bs = "ps", by = sex) +
                         s(duration, bs = "ps", by = sex) +
                         s(capital, bs = "ps", by = sex) + 
                         s(IMC, bs = "ps", by = sex),
                       data = analysis_data, 
                       family = "binomial", 
                       method = "REML", select = T)
  
  prob <- predict(gam_model_lapse, newdata = validation_data, type = "response")

  prob <- as_tibble(prob) %>%
    mutate(lapse = validation_data$lapse)
  
  confusion_matrix <- prob %>%
    mutate(pred_class = as.factor(ifelse(value > 0.03884555, "yes", "no"))) %>%
    conf_mat(truth = lapse, estimate = pred_class)
  
  f2[i] <- confusion_matrix %>% summary(event_level = "second") %>%
    filter(.metric == "f_meas") %>% dplyr::select(.estimate) %>% pull()
  kappa[i] <- confusion_matrix %>% summary(event_level = "second") %>%
    filter(.metric == "kap") %>% dplyr::select(.estimate) %>% pull()
  rec[i] <- confusion_matrix %>% summary(event_level = "second") %>%
    filter(.metric == "recall") %>% dplyr::select(.estimate) %>% pull()
}
t1 <- Sys.time()
t1 - t

mean(f2)
mean(kappa)
mean(rec)

# XGBOOST
set.seed(123)
max_ent_xgboost <- grid_max_entropy(parameters(xgboost_wf), size = 15)

t <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

xgboost_wf_tune <- xgboost_wf %>%
  tune_grid(
    resamples = lapse_folds,
    grid = max_ent_xgboost,
    control = grid_ctrl,
    metrics = my_metrics
    )

stopCluster(cl)
saveRDS(xgboost_wf_tune, "../02_lapse_model/results_lapse/xgboost_wf_tune.RDS")
t3 <- Sys.time()
t3 - t

# Random Forest
set.seed(123)

rf_param <- rf_wf %>%
  parameters() %>%
  update(mtry = finalize(mtry(c(1, 10))))

max_ent_rf <- grid_max_entropy(rf_param, size = 15)

t <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

rf_wf_tune <- rf_wf %>%
  tune_grid(
    resamples = lapse_folds,
    grid = max_ent_rf,
    control = grid_ctrl,
    metrics = my_metrics
    )

saveRDS(rf_wf_tune, "../02_lapse_model/results_lapse/rf_wf_tune.RDS")
stopCluster(cl)

t4 <- Sys.time()
t4 - t


set.seed(123)
max_ent_mlp <- grid_max_entropy(parameters(mlp_wf), size = 10)

t <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

mlp_wf_tune <- mlp_wf %>%
  tune_grid(
    resamples = lapse_folds,
    grid = max_ent_mlp,
    control = grid_ctrl,
    metrics = my_metrics
    )

stopCluster(cl)
saveRDS(mlp_wf_tune, "../02_lapse_model/results_lapse/mlp_wf_tune.RDS")
t5 <- Sys.time()
t5 - t
```


