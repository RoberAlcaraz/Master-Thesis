---
title: '**TFM: Model tuning and selection**'
author: '*Roberto Jes√∫s Alcaraz Molina*'
date: "14/04/2021"
output:
  html_document:
    toc: T
    fig_caption: yes
header-includes: 
- \usepackage{float}
- \usepackage{amsbsy}
- \usepackage{amsmath}
- \usepackage{graphicx}
- \usepackage{subfig}
- \usepackage{booktabs}
---

```{r include=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = F, 
                      message = FALSE,
                      fig.pos="H", 
                      fig.align="center",
                      fig.width=15,
                      cache=FALSE, error = TRUE)
```

```{r, echo = F}
# install.packages("pacman")
pacman::p_load(tidyverse, tidymodels, workflowsets, tune, patchwork, FactoMineR, doParallel, usemodels)
theme_set(theme_bw())

# Models packages
pacman::p_load(ranger)
```

## Code

```{r}
insurance <- readRDS("insurance.RDS")

set.seed(23)
insurance_folds <- vfold_cv(insurance, v = 3, strata = mortality)

# Model
glm_model <- logistic_reg() %>%
  set_engine("glm")

# Workflow
glm_wflow <- workflow() %>%
  add_model(glm_model)
```

To start, we are going to compare multiple models with different features or preprocessing methods but the model for all of them is going to be the same: glm

```{r}
keep_pred <- control_resamples(save_pred = TRUE, 
                               save_workflow = TRUE)

base <- recipe(mortality ~ period + cover + sex + smoker + good_health + actuarial_age + duration + capital + IMC, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_log(capital, IMC, base = 10)

base_1 <- recipe(mortality ~ period + sex + smoker + good_health + actuarial_age + duration + capital + IMC, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_log(capital, IMC, base = 10)

base_2 <- recipe(mortality ~ period + sex + smoker + good_health + actuarial_age + duration + capital, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_log(capital, base = 10)

base_3 <- recipe(mortality ~ period + sex + smoker + good_health + actuarial_age + duration, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes()) 

base_4 <- recipe(mortality ~ period + sex + smoker + good_health + actuarial_age, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes())

cat <- recipe(mortality ~ period + cover + sex + smoker + good_health + actuarial_age + duration + capital_factor_2 + IMC_factor_2, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes())

cat_1 <- recipe(mortality ~ period + cover + sex + smoker + good_health + actuarial_age + duration + IMC_factor_2, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes())

cat_2 <- recipe(mortality ~ period + sex + smoker + good_health + actuarial_age + duration + IMC_factor_2, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes())

cat_3 <- recipe(mortality ~ sex + smoker + good_health + actuarial_age + duration + IMC_factor_2, data = insurance) %>%
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_dummy(all_nominal(), -all_outcomes())

set_predictors <- list(
  # original
  base   = base,
  base_1 = base_1,
  base_2 = base_2, 
  base_3 = base_3,
  base_4 = base_4,
  cat    = cat,  
  cat_1  = cat_1, 
  cat_2  = cat_2, 
  cat_3  = cat_3
)

glm_models <- workflow_set(set_predictors, list(glm = glm_model), cross = F)
glm_models
```

```{r}
glm_models <- 
  glm_models %>% 
  workflow_map("fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 23, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = insurance_folds, control = keep_pred,
               metrics = metric_set(roc_auc, kap))



# i 1 of 9 resampling: base_glm
# v 1 of 9 resampling: base_glm (41.4s)
# i 2 of 9 resampling: base_1_glm
# v 2 of 9 resampling: base_1_glm (40.5s)
# i 3 of 9 resampling: base_2_glm
# v 3 of 9 resampling: base_2_glm (39.2s)
# i 4 of 9 resampling: base_3_glm
# v 4 of 9 resampling: base_3_glm (34.6s)
# i 5 of 9 resampling: base_4_glm
# v 5 of 9 resampling: base_4_glm (32.8s)
# i 6 of 9 resampling: cat_glm
# v 6 of 9 resampling: cat_glm (38.8s)
# i 7 of 9 resampling: cat_1_glm
# v 7 of 9 resampling: cat_1_glm (36.9s)
# i 8 of 9 resampling: cat_2_glm
# v 8 of 9 resampling: cat_2_glm (37.3s)
# i 9 of 9 resampling: cat_3_glm
# v 9 of 9 resampling: cat_3_glm (31.3s)
```

```{r}
collect_metrics(glm_models)
```

```{r}
autoplot(glm_models, metric = "roc_auc")
```


# Machine learning

```{r}
use_xgboost(
  formula = mortality ~ period + cover + sex + smoker + good_health + actuarial_age + duration + capital + IMC, 
  data = insurance, 
  verbose = T, 
  tune = T, 
  colors = T
)
```

```{r}
use_kknn(
  formula = mortality ~ period + cover + sex + smoker + good_health + actuarial_age + duration + capital + IMC, 
  data = insurance, 
  verbose = T, 
  tune = T, 
  colors = T
)
```

```{r}
kknn_recipe <- 
  recipe(formula = mortality ~ period + cover + sex + smoker + good_health + 
    actuarial_age + duration + capital + IMC, data = insurance) %>% 
  themis::step_upsample(mortality, over_ratio = 0.01) %>%
  step_novel(all_nominal_predictors()) %>% 
  ## This model requires the predictors to be numeric. The most common 
  ## method to convert qualitative predictors to numeric is to create 
  ## binary indicator variables (aka dummy variables) from these 
  ## predictors. 
  step_dummy(all_nominal_predictors()) %>% 
  ## Since distance calculations are used, the predictor variables should 
  ## be on the same scale. Before centering and scaling the numeric 
  ## predictors, any predictors with a single unique value are filtered 
  ## out. 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) 

kknn_spec <- 
  nearest_neighbor(neighbors = tune(), weight_func = "rectangular") %>% 
  set_mode("classification") %>% 
  set_engine("kknn") 

kknn_workflow <- 
  workflow() %>% 
  add_recipe(kknn_recipe) %>% 
  add_model(kknn_spec) 

set.seed(77980)
kknn_tune <-
  tune_grid(kknn_workflow, 
            resamples = insurance_folds,
            grid = 3)
```



```{r}
xgboost_recipe <- 
  recipe(formula = mortality ~ period + cover + sex + smoker + good_health + 
    actuarial_age + duration + capital + IMC, data = insurance) %>% 
  step_novel(all_nominal_predictors()) %>% 
  ## This model requires the predictors to be numeric. The most common 
  ## method to convert qualitative predictors to numeric is to create 
  ## binary indicator variables (aka dummy variables) from these 
  ## predictors. However, for this model, binary indicator variables can be 
  ## made for each of the levels of the factors (known as 'one-hot 
  ## encoding'). 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_zv(all_predictors()) 

xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(xgboost_recipe) %>% 
  add_model(xgboost_spec) 

set.seed(56931)
xgboost_tune <-
  xgboost_workflow %>%
  tune_grid(resamples = insurance_folds, 
            grid = 2, 
            metrics = metric_set(roc_auc))
```




```{r}
parsnip_addin()



decision_tree_rpart_spec <-
  decision_tree(tree_depth = tune(), min_n = tune(), cost_complexity = tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')

logistic_reg_glmnet_spec <-
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine('glmnet')

```
















