---
title: '**Mortality: Model tuning and selection**'
author: '*Roberto Jes√∫s Alcaraz Molina*'
date: "14/04/2021"
output:
  html_document:
    toc: T
    fig_caption: yes
header-includes: 
- \usepackage{float}
- \usepackage{amsbsy}
- \usepackage{amsmath}
- \usepackage{graphicx}
- \usepackage{subfig}
- \usepackage{booktabs}
---

```{r include=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = F, 
                      message = FALSE,
                      fig.pos="H", 
                      fig.align="center",
                      fig.width=15,
                      cache=FALSE, error = TRUE)
```

```{r, echo = F}
# install.packages("pacman")
pacman::p_load(tidyverse, tidymodels, workflowsets, tune, patchwork, FactoMineR, doParallel, usemodels)
theme_set(theme_bw())

# Models packages
pacman::p_load(ranger, baguette)
```

## Code

```{r, eval = T}
insurance <- readRDS("../00_data/insurance.RDS")

set.seed(23)
insurance_folds <- vfold_cv(insurance_train, v = 3, 
                            strata = mortality)

rfe_model <- rand_forest() %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("classification")

# Predictive model for the deaths
set.seed(1234)
select_rec <- 
  recipe(mortality ~ period + cover + sex + smoker + good_health +
                   actuarial_age + duration + capital + IMC,
         data = insurance_train) %>%
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 1234) %>%
  step_select_vip(all_predictors(), outcome = "mortality", model = rfe_model, top_p = 7)

# The recursive feature selection with Random Forest discard the variables cover and good_health.
```

To start, we are going to compare multiple models:

```{r}
keep_pred <- control_grid(
  save_pred = TRUE, 
  save_workflow = TRUE
  )
```

```{r}
# GLM and glmnet

glm_recipe <- 
  recipe(formula = mortality ~ period + sex + smoker + actuarial_age + duration + capital + IMC, 
         data = insurance_train) %>% 
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 1234) %>%
  step_log(capital, IMC, base = 10) %>%
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) 

logistic_reg_glm_spec <-
  logistic_reg() %>%
  set_engine('glm')



decision_tree_rpart_spec <-
  decision_tree(tree_depth = tune(), min_n = tune(), cost_complexity = tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')

usemodels::use_glmnet(formula = mortality ~ period + sex + smoker + actuarial_age + duration + capital + IMC, 
         data = insurance_train)

```

```{r, eval=F}
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

# fit here

stopCluster(cl)
```


```{r, eval = F}
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

glm_models <- 
  glm_models %>% 
  workflow_map("fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 23, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = insurance_folds, control = keep_pred,
               metrics = metric_set(roc_auc, kap))

stopCluster(cl)

```


# Machine learning




Let's try different models

```{r}
kknn_recipe <- 
  recipe(formula = mortality ~ period + sex + smoker + actuarial_age + duration + capital + IMC, 
         data = insurance_train) %>% 
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 1234) %>%
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) 

kknn_spec <- 
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn") 

kknn_workflow <- 
  workflow() %>% 
  add_recipe(kknn_recipe) %>% 
  add_model(kknn_spec) 


cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(35790)
kknn_tune <- kknn_workflow %>%
  tune_grid(
    resamples = insurance_folds, 
    grid = 10
    )

stopCluster(cl)
```












