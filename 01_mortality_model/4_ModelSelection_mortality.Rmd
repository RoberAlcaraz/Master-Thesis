---
title: '**Mortality: Model tuning and selection**'
author: '*Roberto Jes√∫s Alcaraz Molina*'
date: "14/04/2021"
output:
  html_document:
    toc: T
    fig_caption: yes
header-includes: 
- \usepackage{float}
- \usepackage{amsbsy}
- \usepackage{amsmath}
- \usepackage{graphicx}
- \usepackage{subfig}
- \usepackage{booktabs}
---

```{r include=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = F, 
                      message = FALSE,
                      fig.pos="H", 
                      fig.align="center",
                      fig.width=15,
                      cache=FALSE, error = TRUE)
```

```{r, echo = F}
# install.packages("pacman")
pacman::p_load(tidymodels, workflowsets, tune, patchwork, FactoMineR, doParallel, usemodels, mgcv)
theme_set(theme_bw())

# Models packages
pacman::p_load(ranger, reticulate, keras, tensorflow)
library(tidyverse, attach.required = T)
```

## Code

```{r, eval = T}
insurance <- readRDS("../00_data/insurance.RDS")

insurance <- insurance %>%
  filter(actuarial_age >= 21, actuarial_age <= 61) %>%
  filter(duration <= 13)


set.seed(123)

insurance_split <- initial_split(insurance, prop = 0.7, strata = mortality)
insurance_train <- training(insurance_split)
insurance_test  <- testing(insurance_split)

set.seed(23)
insurance_folds <- vfold_cv(insurance_train, v = 5, 
                            strata = mortality)

basic_rec <- 
  recipe(mortality ~ sex + smoker + duration + actuarial_age + IMC + capital, 
         data = insurance_train) %>%
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 123) %>%
  step_log(capital, IMC, base = 10) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ duration:sex_woman + duration:smoker_yes +
                  IMC:sex_woman + capital:sex_woman)
```

We are going to compare the following models: gam, xgboost, random forest and neural networks.

```{r}
grid_ctrl <- control_grid(
  save_pred = TRUE, 
  save_workflow = TRUE, 
  event_level = "second"
  )

f_meas_2 <- function(data, truth, estimate, na_rm = TRUE, ...){
  f_meas(
    data = data,
    truth = !! rlang::enquo(truth),
    estimate = !! rlang::enquo(estimate),
    # set beta = 2
    beta = 2,
    na_rm = TRUE,
    ...
  )
}

f_meas_2 <- new_class_metric(f_meas_2, direction = "maximize")

# It works
# prob %>%
#     mutate(pred_class = as.factor(ifelse(value > t[i], "yes", "no"))) %>%
#     f_meas(mortality, pred_class, beta = 2, event_level = "second")
# prob %>%
#     mutate(pred_class = as.factor(ifelse(value > t[i], "yes", "no"))) %>%
#     f_meas_2(mortality, pred_class)

my_metrics = metric_set(f_meas_2, recall, kap)
```


# Models


```{r}
xgboost_rec <- 
  recipe(mortality ~ sex + smoker + duration + actuarial_age + IMC + capital, 
         data = insurance_train) %>%
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 123) %>% 
  step_log(capital, IMC, base = 10) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  step_interact(terms = ~ duration:sex_man + duration:sex_woman + duration:smoker_yes +
                  duration:smoker_no + IMC:sex_man + IMC:sex_woman + capital:sex_man +
                  capital:sex_woman) %>%
  step_zv(all_predictors())

xgboost_model <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
    loss_reduction = tune(), sample_size = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 

xgboost_wf <- 
  workflow() %>% 
  add_recipe(xgboost_rec) %>% 
  add_model(xgboost_model) 
```

```{r}
# Random Forest
rf_rec <- 
  recipe(mortality ~ sex + smoker + duration + actuarial_age + IMC + capital, 
         data = insurance_train) %>%
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 123) %>%
  step_log(capital, IMC, base = 10) %>%
  step_interact(terms = ~ duration:sex + duration:smoker +
                  IMC:sex + capital:sex)

rf_model <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

rf_wf <- 
  workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_model)
```


```{r}
mlp_rec <- 
  recipe(mortality ~ sex + smoker + duration + actuarial_age + IMC + capital, 
         data = insurance_train) %>%
  themis::step_upsample(mortality, over_ratio = 0.01, seed = 123) %>%
  step_log(capital, IMC, base = 10) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ duration:sex + duration:smoker +
                  IMC:sex + capital:sex) %>% 
  step_zv(all_predictors()) %>%
  step_corr(all_predictors())

mlp_model <-
  mlp(hidden_units = tune(), epochs = tune(), penalty = tune()) %>%
  set_engine('nnet') %>%
  set_mode('classification')

mlp_wf <- 
  workflow() %>%
  add_recipe(mlp_rec) %>%
  add_model(mlp_model)
```


# Parameter tuning 


```{r}
# GAM
n <- nrow(insurance_folds)

f2 <- seq(1, n)
kappa <- seq(1, n)
rec <- seq(1, n)
t <- Sys.time()

for (i in 1:n){
  
  analysis_data <- analysis(insurance_folds$splits[[i]])
  analysis_data <- 
    recipe(mortality ~ sex + smoker + actuarial_age + duration + capital + IMC,
           data = analysis_data) %>%
    themis::step_upsample(mortality, over_ratio = 0.01, seed = 123) %>%
    step_log(capital, IMC, base = 10) %>% 
    prep() %>% juice()
  
  validation_data <- assessment(insurance_folds$splits[[i]])
  
  gam_model <- gam(mortality ~ sex + smoker +
                   s(actuarial_age, bs = "ps") + s(duration, bs = "ps") +
                   s(capital, bs = "ps") + s(IMC, bs = "ps") +
                   s(duration, bs = "ps", by = sex) +
                   s(duration, bs = "ps", by = smoker) +
                   s(capital, bs = "ps", by = sex) +
                   s(IMC, bs = "ps", by = sex),
                 data = analysis_data, family = "binomial", method = "REML", select = T)
  
  prob <- predict(gam_model, newdata = validation_data, type = "response")

  prob <- as_tibble(prob) %>%
    mutate(mortality = validation_data$mortality)
  
  confusion_matrix <- prob %>%
    mutate(pred_class = as.factor(ifelse(value > 0.03884555, "yes", "no"))) %>%
    conf_mat(truth = mortality, estimate = pred_class)
  
  f2[i] <- confusion_matrix %>% summary(event_level = "second", beta = 2) %>%
    filter(.metric == "f_meas") %>% dplyr::select(.estimate) %>% pull()
  kappa[i] <- confusion_matrix %>% summary(event_level = "second") %>%
    filter(.metric == "kap") %>% dplyr::select(.estimate) %>% pull()
  rec[i] <- confusion_matrix %>% summary(event_level = "second") %>%
    filter(.metric == "recall") %>% dplyr::select(.estimate) %>% pull()
}
t1 <- Sys.time()
t1 - t

mean(f2)
mean(kappa)
mean(rec)

# XGBOOST
set.seed(123)
max_ent_xgboost <- grid_max_entropy(parameters(xgboost_wf), size = 15)

t <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

xgboost_wf_tune <- xgboost_wf %>%
  tune_grid(
    resamples = insurance_folds,
    grid = max_ent_xgboost,
    control = grid_ctrl,
    metrics = my_metrics
    )

stopCluster(cl)
saveRDS(xgboost_wf_tune, "../01_mortality_model/results_mortality/xgboost_wf_tune.RDS")
t3 <- Sys.time()
t3 - t
# Time difference of 4.086126 hours

# Random Forest
set.seed(123)

rf_param <- rf_wf %>%
  parameters() %>%
  update(mtry = finalize(mtry(c(1, 10))))

max_ent_rf <- grid_max_entropy(rf_param, size = 15)

t <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

rf_wf_tune <- rf_wf %>%
  tune_grid(
    resamples = insurance_folds,
    grid = max_ent_rf,
    control = grid_ctrl,
    metrics = my_metrics
    )

saveRDS(rf_wf_tune, "../01_mortality_model/results_mortality/rf_wf_tune.RDS")
stopCluster(cl)

t4 <- Sys.time()
t4 - t
# Time difference of 5.317373 hours


set.seed(123)
max_ent_mlp <- grid_max_entropy(parameters(mlp_wf), size = 10)

t <- Sys.time()
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

mlp_wf_tune <- mlp_wf %>%
  tune_grid(
    resamples = insurance_folds,
    grid = max_ent_mlp,
    control = grid_ctrl,
    metrics = my_metrics
    )

stopCluster(cl)
saveRDS(mlp_wf_tune, "../01_mortality_model/results_mortality/mlp_wf_tune.RDS")
t5 <- Sys.time()
t5 - t
```


```{r}
autoplot(xgboost_wf_tune)

xgboost_wf_tune %>%
  collect_predictions() %>%
  conf_mat(truth = mortality,
           estimate = .pred_class)

rf_wf_tune %>%
  collect_predictions() %>%
  conf_mat(truth = mortality,
           estimate = .pred_class)

xgboost_param <- xgboost_wf_tune %>%
  collect_metrics() %>%
  filter(.metric == "f_meas") %>%
  arrange(desc(mean)) %>%
  slice(1) %>%
  dplyr::select(trees, min_n, tree_depth, learn_rate, loss_reduction, sample_size)

xgboost_wf <- xgboost_wf %>%
  finalize_workflow(xgboost_param)

xgboost_final_wf <- xgboost_wf %>%
  last_fit(insurance_split)

xgboost_final_wf %>%
  collect_metrics()

xgboost_final_wf %>%
  collect_predictions() %>%
  conf_mat(truth = mortality,
           estimate = .pred_class) %>%
  summary(event_level = "second", beta = 2)
```


```{r}
mlp_param <- mlp_wf_tune %>%
  collect_metrics() %>%
  filter(.metric == "f_meas") %>%
  arrange(desc(mean)) %>%
  slice(1) %>%
  dplyr::select(hidden_units, epochs, activation)

mlp_param <- tibble(
  hidden_units = 9,
  epochs = 210,
  activation = "linear"
)

autoplot(mlp_wf_tune)
2
mlp_wf <- mlp_wf %>%
  finalize_workflow(mlp_param)

final_wf <- mlp_wf %>%
  last_fit(insurance_split)

final_wf %>%
  collect_predictions() %>%
  conf_mat(truth = mortality,
           estimate = .pred_class) %>%
  summary(event_level = "second", beta = 2)
```




```{r, eval=F}
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

# fit here

stopCluster(cl)
```

